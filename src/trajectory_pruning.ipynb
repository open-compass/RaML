{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Demo For Trajectory Pruning\n",
    "This file shows the basic implementation of the experiments of the trajectory pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/QwQ-32B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load AIME24 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingFaceH4/aime_2024\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Original Trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define verify function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(predictions, references):\n",
    "    try:\n",
    "        from latex2sympy2_extended import NormalizationConfig\n",
    "        from math_verify import (ExprExtractionConfig,\n",
    "                                    LatexExtractionConfig, parse, verify)\n",
    "    except ImportError:\n",
    "        raise ImportError('Failed to import required modules. Please '\n",
    "                            'install the necessary packages: '\n",
    "                            'pip install math_verify latex2sympy2_extended')\n",
    "\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    details = []\n",
    "    for i, j in zip(predictions, references):\n",
    "        count += 1\n",
    "        j_with_env = f'${j}$'\n",
    "        gold_parsed = parse(\n",
    "            j_with_env,\n",
    "            extraction_mode='first_match',\n",
    "            extraction_config=[\n",
    "                LatexExtractionConfig(),\n",
    "                ExprExtractionConfig(),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        if len(gold_parsed) != 0:\n",
    "            # We require the answer to be provided in correct\n",
    "            # latex (no malformed operators)\n",
    "            answer_parsed = parse(\n",
    "                i,\n",
    "                extraction_config=[\n",
    "                    LatexExtractionConfig(\n",
    "                        normalization_config=NormalizationConfig(\n",
    "                            nits=False,\n",
    "                            malformed_operators=False,\n",
    "                            basic_latex=True,\n",
    "                            equations=True,\n",
    "                            boxed='all',\n",
    "                            units=True,\n",
    "                        ),\n",
    "                        # Ensures that boxed is tried first\n",
    "                        boxed_match_priority=0,\n",
    "                        try_extract_without_anchor=False,\n",
    "                    )\n",
    "                ],\n",
    "                extraction_mode='first_match',\n",
    "            )\n",
    "\n",
    "            answer_correct = float(verify(answer_parsed, gold_parsed))\n",
    "            correct += answer_correct\n",
    "            detail = {\n",
    "                'pred': str(answer_parsed),\n",
    "                'answer': str(gold_parsed),\n",
    "                'correct': True if answer_correct else False,\n",
    "            }\n",
    "            details.append(detail)\n",
    "    return details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLM(\"Qwen/Qwen3-32B\", tensor_parallel_size=2, gpu_memory_utilization=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"{question}\\n\\nPlease reason step by step, and put your final answer within \\\\boxed{{}}\"\n",
    "inputs = []\n",
    "answers = []\n",
    "questions = []\n",
    "for example in dataset:\n",
    "    questions.append(example[\"problem\"])\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt_template.format(question=questions[-1])}\n",
    "    ]\n",
    "    inputs.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=True))\n",
    "    answers.append(example[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate response, only one sample for simple demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 16/16 [08:58<00:00, 33.64s/it, est. speed input: 4.70 toks/s, output: 130.23 toks/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    max_tokens=32768,  \n",
    "    temperature=0.6,  \n",
    "    top_p=0.95,\n",
    "    min_p=0.0,\n",
    "    top_k=40,\n",
    "    n=16,\n",
    "    skip_special_tokens=False,\n",
    ")\n",
    "outputs = model.generate(inputs[:1], sampling_params, use_tqdm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parse and evaluate the trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "trajectories = []\n",
    "references = []\n",
    "for output, answer in zip(outputs, answers[:1]):\n",
    "    prompt = output.prompt\n",
    "    for r in output.outputs:\n",
    "        trajectories.append(r.text)\n",
    "        references.append(answer)\n",
    "scores = score(trajectories, references)\n",
    "\n",
    "print(\"mean accuracy: \", np.mean([score[\"correct\"] for score in scores]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Summarized Trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load summarization model, we utilize Qwen3-32B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 07:22:29 [config.py:583] This model supports multiple tasks: {'generate', 'embed', 'score', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 05-26 07:22:29 [config.py:1515] Defaulting to use mp for distributed inference\n",
      "INFO 05-26 07:22:29 [config.py:1693] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-26 07:22:34 [__init__.py:256] Automatically detected platform cuda.\n",
      "[2025-05-26 07:22:36,005] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "INFO 05-26 07:22:36 [core.py:53] Initializing a V1 LLM engine (v0.8.1) with config: model='Qwen/Qwen2.5-32B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-32B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-32B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-26 07:22:36 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 05-26 07:22:36 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_07c33891'), local_subscribe_addr='ipc:///tmp/11f960ae-63d2-4fc4-95e3-d691efb28f37', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 05-26 07:22:40 [__init__.py:256] Automatically detected platform cuda.\n",
      "[2025-05-26 07:22:42,037] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "WARNING 05-26 07:22:43 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fd2682a5050>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:22:43 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c58d7188'), local_subscribe_addr='ipc:///tmp/e42a919d-cebe-453f-a9d4-028a15132d4f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 05-26 07:22:47 [__init__.py:256] Automatically detected platform cuda.\n",
      "[2025-05-26 07:22:48,506] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "WARNING 05-26 07:22:49 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f043100e690>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:22:49 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d541aa50'), local_subscribe_addr='ipc:///tmp/c7474b9b-4bfd-403d-8de6-58cacf627fa7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:22:49 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:22:49 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:22:49 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:22:49 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:22:50 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 05-26 07:22:50 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:22:50 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_0a5c508e'), local_subscribe_addr='ipc:///tmp/64ba8c88-c697-4772-b9ee-ec0b9890d01d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:22:50 [parallel_state.py:967] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-26 07:22:50 [parallel_state.py:967] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:22:50 [cuda.py:215] Using Flash Attention backend on V1 engine.\n",
      "INFO 05-26 07:22:50 [cuda.py:215] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:22:50 [gpu_model_runner.py:1164] Starting to load model Qwen/Qwen2.5-32B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:22:50 [gpu_model_runner.py:1164] Starting to load model Qwen/Qwen2.5-32B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m WARNING 05-26 07:22:50 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m WARNING 05-26 07:22:50 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:22:51 [weight_utils.py:257] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:22:51 [weight_utils.py:257] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:04<01:12,  4.53s/it]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:09<01:11,  4.76s/it]\n",
      "Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:13<01:02,  4.45s/it]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:17<00:57,  4.40s/it]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:22<00:52,  4.35s/it]\n",
      "Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:26<00:47,  4.33s/it]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:30<00:43,  4.30s/it]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:35<00:38,  4.32s/it]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:40<00:36,  4.56s/it]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 10/17 [00:44<00:32,  4.66s/it]\n",
      "Loading safetensors checkpoint shards:  65% Completed | 11/17 [00:49<00:26,  4.47s/it]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 12/17 [00:53<00:22,  4.55s/it]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 13/17 [00:58<00:18,  4.68s/it]\n",
      "Loading safetensors checkpoint shards:  82% Completed | 14/17 [01:03<00:14,  4.69s/it]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 15/17 [01:08<00:09,  4.69s/it]\n",
      "Loading safetensors checkpoint shards:  94% Completed | 16/17 [01:13<00:04,  4.84s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [01:17<00:00,  4.74s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [01:17<00:00,  4.58s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:24:09 [loader.py:429] Loading weights took 77.91 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:24:09 [loader.py:429] Loading weights took 77.35 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:24:10 [gpu_model_runner.py:1176] Model loading took 30.7098 GB and 79.461885 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:24:10 [gpu_model_runner.py:1176] Model loading took 30.7098 GB and 79.465753 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:24:24 [backends.py:409] Using cache directory: /root/.cache/vllm/torch_compile_cache/4f59ca9ae3/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:24:24 [backends.py:419] Dynamo bytecode transform time: 14.91 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:24:25 [backends.py:409] Using cache directory: /root/.cache/vllm/torch_compile_cache/4f59ca9ae3/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:24:25 [backends.py:419] Dynamo bytecode transform time: 14.95 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:24:28 [backends.py:132] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:24:28 [backends.py:132] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:25:16 [backends.py:144] Compiling a graph for general shape takes 50.66 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:25:17 [backends.py:144] Compiling a graph for general shape takes 51.20 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:25:42 [monitor.py:33] torch.compile takes 65.57 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:25:42 [monitor.py:33] torch.compile takes 66.15 s in total\n",
      "INFO 05-26 07:25:43 [kv_cache_utils.py:537] GPU KV cache size: 146,848 tokens\n",
      "INFO 05-26 07:25:43 [kv_cache_utils.py:540] Maximum concurrency for 32,768 tokens per request: 4.48x\n",
      "INFO 05-26 07:25:43 [kv_cache_utils.py:537] GPU KV cache size: 146,848 tokens\n",
      "INFO 05-26 07:25:43 [kv_cache_utils.py:540] Maximum concurrency for 32,768 tokens per request: 4.48x\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:26:20 [custom_all_reduce.py:229] Registering 8643 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:26:27 [custom_all_reduce.py:229] Registering 8643 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:26:27 [gpu_model_runner.py:1499] Graph capturing finished in 44 secs, took 3.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:26:27 [gpu_model_runner.py:1499] Graph capturing finished in 44 secs, took 3.56 GiB\n",
      "INFO 05-26 07:26:27 [core.py:138] init engine (profile, create kv cache, warmup model) took 137.88 seconds\n"
     ]
    }
   ],
   "source": [
    "sum_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-32B-Instruct\")\n",
    "sum_model = LLM(\"Qwen/Qwen2.5-32B-Instruct\", trust_remote_code=True, tensor_parallel_size=2, gpu_memory_utilization=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_prompt_template = \"{trajectory}\\n\\n\\nSummarize the aforementioned reasoning process and not explicitly include the final conclusion and answer. Only provide the English summary.\"\n",
    "\n",
    "summarize_messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": summarize_prompt_template.format(\n",
    "            # remove answer\n",
    "            trajectory=sum_tokenizer.decode(sum_tokenizer.encode(trajectory.split(\"</think>\")[0].split(\"\\n\\n**Final Answer**\")[0])[:24576])\n",
    "        )},\n",
    "    ] for trajectory in trajectories\n",
    "]\n",
    "summarize_prompts = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True) for message in summarize_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate summarized trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 16/16 [00:31<00:00,  1.94s/it, est. speed input: 1829.23 toks/s, output: 217.19 toks/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(n=1, max_tokens=8192, seed=42, temperature=1.0)\n",
    "request_outputs = sum_model.generate(summarize_prompts, sampling_params, use_tqdm=True) \n",
    "summarized_trajectories = [output.text for resquest_output in request_outputs for output in resquest_output.outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay, let\\'s see. So, Aya goes for a 9-kilometer walk every morning and stops at a coffee shop. The problem gives me two scenarios where her walking speed affects the total time, including the time spent at the coffee shop, which is the same t minutes in both cases. Then, they want me to find out how long it takes her when she walks at s + 1/2 km/h, again including the t minutes at the coffee shop. \\n\\nFirst, let me parse the information. \\n\\nIn the first scenario, when she walks at a constant speed of s km/h, the total time (walking + coffee shop) is 4 hours. In the second scenario, when she walks at s + 2 km/h, the total time is 2 hours and 24 minutes. The coffee shop time t is the same in both cases. \\n\\nSo, I need to find t and s first, and then use those to calculate the total time when her speed is s + 1/2. \\n\\nLet me start by converting all the times to hours or all to minutes? Probably hours since her speed is in km per hour. Let me check:\\n\\nFirst scenario: total time is 4 hours, which includes t minutes at the coffee shop. So, the walking time is 4 hours minus t minutes. But I need to convert t minutes to hours. Similarly, the second scenario: total time is 2 hours and 24 minutes, which is 2 + 24/60 = 2.4 hours. Again, the walking time would be 2.4 hours minus t minutes (converted to hours). \\n\\nAlternatively, maybe I can write equations for the walking time. Let me think. \\n\\nLet me denote:\\n\\nLet’s let t be in hours. Wait, but the problem says t minutes. Wait, the problem says \"including t minutes spent in the coffee shop\". So t is in minutes. So in the first case, total time is 4 hours, which is 240 minutes. So walking time is 240 - t minutes. Similarly, second case: total time is 2 hours 24 minutes, which is 144 minutes. Therefore, walking time is 144 - t minutes.\\n\\nBut maybe working with hours is better. Let me think. Let me convert t minutes to hours. Since t minutes is t/60 hours.\\n\\nSo, for the first scenario:\\n\\nTotal time: 4 hours = walking time + t/60 hours.\\n\\nSimilarly, second scenario:\\n\\nTotal time: 2 hours 24 minutes = 2 + 24/60 = 2.4 hours = walking time + t/60 hours.\\n\\nTherefore, the walking time in the first scenario is 4 - t/60 hours, and in the second scenario is 2.4 - t/60 hours.\\n\\nBut walking time can also be calculated as distance divided by speed. The distance is 9 km in both cases.\\n\\nSo, for the first scenario: walking time = 9 / s = 4 - t/60.\\n\\nFor the second scenario: walking time = 9 / (s + 2) = 2.4 - t/60.\\n\\nTherefore, we have two equations:\\n\\n1) 9/s = 4 - t/60\\n\\n2) 9/(s + 2) = 2.4 - t/60\\n\\nSo, we have two equations with two variables: s and t. We can solve for s and t.\\n\\nLet me write them again:\\n\\nEquation 1: 9/s + t/60 = 4\\n\\nEquation 2: 9/(s + 2) + t/60 = 2.4\\n\\nIf I subtract equation 2 from equation 1, the t/60 terms will cancel out. Let me try that.\\n\\nEquation 1 - Equation 2:\\n\\n[9/s + t/60] - [9/(s + 2) + t/60] = 4 - 2.4\\n\\nSimplify:\\n\\n9/s - 9/(s + 2) = 1.6\\n\\nSo, 9 [ 1/s - 1/(s + 2) ] = 1.6\\n\\nCompute 1/s - 1/(s + 2) = [ (s + 2) - s ] / [s(s + 2) ] = 2 / [s(s + 2)]\\n\\nTherefore:\\n\\n9 * [2 / (s(s + 2))] = 1.6\\n\\nSimplify:\\n\\n18 / [s(s + 2)] = 1.6\\n\\nThen, solving for s(s + 2):\\n\\ns(s + 2) = 18 / 1.6\\n\\nCalculate 18 / 1.6:\\n\\nDivide numerator and denominator by 0.2: 18 / 1.6 = 90 / 8 = 11.25\\n\\nAlternatively, 1.6 * 10 = 16, 1.6 * 11 = 17.6, 1.6 * 11.25 = 1.6 * 11 + 1.6 * 0.25 = 17.6 + 0.4 = 18. Correct. So s(s + 2) = 11.25\\n\\nTherefore, s^2 + 2s - 11.25 = 0\\n\\nMultiply both sides by 4 to eliminate decimal:\\n\\nWait, maybe solve quadratic equation. Let me write:\\n\\ns^2 + 2s - 11.25 = 0\\n\\nUsing quadratic formula:\\n\\ns = [ -2 ± sqrt( (2)^2 - 4 * 1 * (-11.25) ) ] / (2 * 1)\\n\\nCompute discriminant:\\n\\n4 + 45 = 49\\n\\nWait:\\n\\nDiscriminant D = 4 + 45? Let me check:\\n\\n(2)^2 = 4, 4 * 1 * 11.25 = 45. Since it\\'s -4ac, and c is -11.25, so -4*(1)*(-11.25) = +45. Therefore, D = 4 + 45 = 49. Perfect square. \\n\\nTherefore, s = [ -2 ± sqrt(49) ] / 2 = [ -2 ± 7 ] / 2\\n\\nWe have two solutions:\\n\\ns = (-2 + 7)/2 = 5/2 = 2.5\\n\\ns = (-2 - 7)/2 = -9/2 = -4.5\\n\\nBut speed can\\'t be negative, so s = 2.5 km/h.\\n\\nOkay, so s is 2.5 km/h. Now we can find t.\\n\\nLet me use equation 1: 9/s + t/60 = 4.\\n\\ns = 2.5, so 9 / 2.5 = ?\\n\\nWell, 2.5 * 3 = 7.5, 2.5 * 3.6 = 9. Because 2.5 * 3 = 7.5, 2.5 * 0.6 = 1.5, so 7.5 + 1.5 = 9. So 9 / 2.5 = 3.6 hours.\\n\\nSo, 3.6 + t / 60 = 4. Therefore, t / 60 = 4 - 3.6 = 0.4 hours. Therefore, t = 0.4 * 60 = 24 minutes. \\n\\nLet me check with equation 2 to make sure.\\n\\nEquation 2: 9 / (s + 2) + t / 60 = 2.4\\n\\ns + 2 = 2.5 + 2 = 4.5 km/h.\\n\\n9 / 4.5 = 2 hours. Then t / 60 is 24 / 60 = 0.4 hours. So total time is 2 + 0.4 = 2.4 hours, which is 2 hours 24 minutes. Correct. So that checks out.\\n\\nSo, s = 2.5 km/h, t = 24 minutes.\\n\\nNow, the question asks: Suppose Aya walks at s + 1/2 km/h. Find the number of minutes the walk takes her, including the t minutes spent in the coffee shop.\\n\\nSo, s + 1/2 = 2.5 + 0.5 = 3.0 km/h.\\n\\nSo, we need to calculate the total time, which is walking time + t minutes.\\n\\nBut walking time is distance / speed. Distance is 9 km. Speed is 3.0 km/h. So walking time is 9 / 3 = 3 hours. Then add t minutes, which is 24 minutes. But wait, need to convert 3 hours to minutes? Wait, total time is walking time (in hours) + t (in minutes). Wait, but we need to have consistent units.\\n\\nWait, no. Wait, the total time is walking time plus coffee shop time. The problem says \"the walk takes her... including t minutes spent in the coffee shop\". So, when they say \"the walk takes her...\", they mean the total time from starting to walk until finishing, including the coffee shop. So, the total time is walking time + coffee time. \\n\\nBut in previous equations, when we had 4 hours, that was walking time (in hours) + t (converted to hours). Similarly, when we calculate for the new speed, we need to compute walking time (in hours) + t (converted to hours) and then convert total time to minutes if needed.\\n\\nBut since t is 24 minutes, maybe it\\'s easier to compute walking time in hours, convert that to minutes, add t minutes, and get total time in minutes.\\n\\nAlternatively, let\\'s do it step by step.\\n\\nFirst, walking speed is 3.0 km/h. Walking time is 9 / 3 = 3 hours. Convert that to minutes: 3 * 60 = 180 minutes. Then add t minutes, which is 24 minutes. Total time is 180 + 24 = 204 minutes. \\n\\nBut let me check with another approach to make sure. Let me use the same structure as previous equations.\\n\\nTotal time = walking time + t / 60 hours. So, walking time is 9 / 3 = 3 hours. Then total time is 3 + 24 / 60 = 3 + 0.4 = 3.4 hours. Convert 3.4 hours to minutes: 3 * 60 + 0.4 * 60 = 180 + 24 = 204 minutes. Yep, same result. \\n\\nTherefore, the answer is 204 minutes. Wait, but let me check if I made any miscalculations. \\n\\nWait, original s was 2.5 km/h. Let me check the first case again. Walking time was 9 / 2.5 = 3.6 hours. 3.6 hours is 3 hours and 36 minutes. Then adding t = 24 minutes gives total time of 3 hours 36 minutes + 24 minutes = 3 hours 60 minutes = 4 hours. Correct. \\n\\nSecond case: walking speed s + 2 = 4.5 km/h. Walking time 9 / 4.5 = 2 hours. Adding 24 minutes gives 2 hours 24 minutes. Correct. \\n\\nThird case: walking speed 3 km/h. Walking time 3 hours. Add 24 minutes: 3 hours 24 minutes, which is 3*60 + 24 = 204 minutes. So yes, that\\'s correct.\\n\\nTherefore, the answer is 204. \\n\\nBut let me check once again if I didn\\'t make a mistake in solving the quadratic equation. \\n\\nOriginal quadratic equation was s^2 + 2s - 11.25 = 0. \\n\\nSolutions were s = [-2 ± sqrt(4 + 45)] / 2 = [-2 ± 7]/2. So positive solution is (5)/2 = 2.5. Correct. \\n\\nYes, so s is 2.5. Then t was 24 minutes. \\n\\nSo, all steps seem correct. Therefore, the answer is 204 minutes. \\n\\n**Final Answer**\\n\\\\boxed{204}\\n</think>\\n\\nTo solve this problem, we are given two scenarios involving Aya’s morning walk and the time she spends at a coffee shop. Our goal is to find the total time (walking + coffee shop) when Aya walks at a different speed.\\n\\n---\\n\\n### Step 1: Define Variables and Convert Units\\n\\nLet:\\n- $ s $ be Aya’s walking speed in km/h.\\n- $ t $ be the time (in minutes) she spends at the coffee shop.\\n\\nWe are told:\\n- When walking at speed $ s $, the **total time** is 4 hours.\\n- When walking at speed $ s + 2 $, the **total time** is 2 hours and 24 minutes = $ 2.4 $ hours.\\n\\nLet’s express the walking time in each case as:\\n$$\\n\\\\text{Walking time} = \\\\frac{9}{s}, \\\\quad \\\\text{Walking time} = \\\\frac{9}{s + 2}\\n$$\\nThen the total time for each case is:\\n$$\\n\\\\text{Total time} = \\\\text{Walking time} + \\\\frac{t}{60} \\\\text{ hours}\\n$$\\n\\nSo we can write the following equations:\\n\\n1. $ \\\\frac{9}{s} + \\\\frac{t}{60} = 4 $\\n2. $ \\\\frac{9}{s + 2} + \\\\frac{t}{60} = 2.4 $\\n\\n---\\n\\n### Step 2: Eliminate $ t $ and Solve for $ s $\\n\\nSubtract the second equation from the first:\\n$$\\n\\\\frac{9}{s} - \\\\frac{9}{s + 2} = 4 - 2.4 = 1.6\\n$$\\n\\nSimplify the left-hand side:\\n$$\\n9 \\\\left( \\\\frac{1}{s} - \\\\frac{1}{s + 2} \\\\right) = 1.6\\n$$\\n$$\\n9 \\\\cdot \\\\frac{2}{s(s + 2)} = 1.6 \\\\quad \\\\Rightarrow \\\\quad \\\\frac{18}{s(s + 2)} = 1.6\\n$$\\n$$\\ns(s + 2) = \\\\frac{18}{1.6} = 11.25\\n$$\\n\\nNow solve the quadratic:\\n$$\\ns^2 + 2s - 11.25 = 0\\n$$\\n\\nUse the quadratic formula:\\n$$\\ns = \\\\frac{-2 \\\\pm \\\\sqrt{4 + 45}}{2} = \\\\frac{-2 \\\\pm 7}{2}\\n$$\\n$$\\ns = 2.5 \\\\quad \\\\text{(since speed cannot be negative)}\\n$$\\n\\n---\\n\\n### Step 3: Solve for $ t $\\n\\nUse the first equation:\\n$$\\n\\\\frac{9}{2.5} + \\\\frac{t}{60} = 4\\n$$\\n$$\\n3.6 + \\\\frac{t}{60} = 4 \\\\quad \\\\Rightarrow \\\\quad \\\\frac{t}{60} = 0.4 \\\\quad \\\\Rightarrow \\\\quad t = 24 \\\\text{ minutes}\\n$$\\n\\n---\\n\\n### Step 4: Find Total Time at $ s + \\\\frac{1}{2} = 3.0 $ km/h\\n\\nWalking time at 3.0 km/h:\\n$$\\n\\\\frac{9}{3} = 3 \\\\text{ hours}\\n$$\\n\\nConvert to minutes:\\n$$\\n3 \\\\text{ hours} = 180 \\\\text{ minutes}\\n$$\\n\\nAdd coffee shop time:\\n$$\\n180 + 24 = \\\\boxed{204} \\\\text{ minutes}\\n$$\\n\\n---\\n\\n### Final Answer\\n\\n$$\\n\\\\boxed{204}\\n$$'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trajectories[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The problem gives two scenarios with Aya walking at different speeds and needing to account for a fixed time t in the coffee shop. The goal is to determine the total time taken when she walks at a speed of s + 1/2 km/h, including the time spent at the coffee shop. The initial steps involve converting the total times given in the scenarios (4 hours and 2 hours and 24 minutes) into equations involving the walking speeds. To create these equations, we use the relationship between speed, distance, and time, where the distance is fixed at 9 km in both cases.\\n\\nThe first scenario's equation is 9/s = 4 - t/60 and the second scenario's equation is 9/(s + 2) = 2.4 - t/60. By subtracting the second equation from the first, the t/60 terms cancel, leaving us with an equation to solve for s. After solving, we find that s = 2.5 km/h. Plugging s back into one of the initial equations allows us to solve for t, giving us t = 24 minutes.\\n\\nFor the final scenario where she walks at s + 1/2 km/h (making the speed 3 km/h), we find the walking time by using the distance-time relationship, which yields 9 km / 3 km/h = 3 hours. Converting 3 hours into minutes (180 minutes) and adding t (24 minutes), we conclude the total time in minutes.\\n\\nThe solution process involves solving the quadratic equation derived from the initial given times to find the speed and subsequently the coffee shop time, and then applying this information to the new scenario to determine the revised total time including the coffee shop time.\\n</think>\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_trajectories[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sum_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Answer Based on the Summarized Trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 07:41:50 [config.py:583] This model supports multiple tasks: {'generate', 'embed', 'score', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 05-26 07:41:50 [config.py:1515] Defaulting to use mp for distributed inference\n",
      "INFO 05-26 07:41:50 [config.py:1693] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-26 07:41:56 [__init__.py:256] Automatically detected platform cuda.\n",
      "[2025-05-26 07:41:57,696] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "INFO 05-26 07:41:58 [core.py:53] Initializing a V1 LLM engine (v0.8.1) with config: model='Qwen/Qwen3-32B', speculative_config=None, tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen3-32B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-26 07:41:58 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 05-26 07:41:58 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_09830435'), local_subscribe_addr='ipc:///tmp/3033cb3b-7fab-43cf-b70b-fef730fa2a53', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 05-26 07:42:02 [__init__.py:256] Automatically detected platform cuda.\n",
      "[2025-05-26 07:42:03,785] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "WARNING 05-26 07:42:04 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f434e26d050>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:04 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d201ed15'), local_subscribe_addr='ipc:///tmp/65314d8e-b3ba-4e0a-94ac-29e3ad5100fc', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 05-26 07:42:08 [__init__.py:256] Automatically detected platform cuda.\n",
      "[2025-05-26 07:42:10,189] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "WARNING 05-26 07:42:11 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fce5eff3c50>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:11 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0d32e852'), local_subscribe_addr='ipc:///tmp/b783c30e-41dd-4b5a-a5ae-fe064ed2f00e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:11 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:11 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:11 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:11 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:11 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:11 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:11 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_7879d013'), local_subscribe_addr='ipc:///tmp/a9cef19b-5fa1-4dd9-8fdc-efab1737a28b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:11 [parallel_state.py:967] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:11 [parallel_state.py:967] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:11 [cuda.py:215] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:11 [cuda.py:215] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:11 [gpu_model_runner.py:1164] Starting to load model Qwen/Qwen3-32B...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:11 [gpu_model_runner.py:1164] Starting to load model Qwen/Qwen3-32B...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m WARNING 05-26 07:42:11 [utils.py:79] Qwen3ForCausalLM has no vLLM implementation, falling back to Transformers implementation. Some features may not be supported and performance may not be optimal.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m WARNING 05-26 07:42:11 [utils.py:79] Qwen3ForCausalLM has no vLLM implementation, falling back to Transformers implementation. Some features may not be supported and performance may not be optimal.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:11 [transformers.py:115] Using Transformers backend.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:11 [transformers.py:115] Using Transformers backend.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m WARNING 05-26 07:42:12 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m WARNING 05-26 07:42:12 [config.py:3670] `torch.compile` is turned on, but the model Qwen/Qwen3-32B does not support it. Please open an issue on GitHub if you want it to be supported.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m WARNING 05-26 07:42:12 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m WARNING 05-26 07:42:12 [config.py:3670] `torch.compile` is turned on, but the model Qwen/Qwen3-32B does not support it. Please open an issue on GitHub if you want it to be supported.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:14 [weight_utils.py:257] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:15 [weight_utils.py:257] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:01<00:19,  1.21s/it]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:02<00:18,  1.22s/it]\n",
      "Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:03<00:14,  1.05s/it]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:04<00:14,  1.10s/it]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:05<00:13,  1.15s/it]\n",
      "Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:06<00:12,  1.17s/it]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:08<00:11,  1.19s/it]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:09<00:10,  1.19s/it]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:10<00:09,  1.19s/it]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 10/17 [00:11<00:08,  1.18s/it]\n",
      "Loading safetensors checkpoint shards:  65% Completed | 11/17 [00:12<00:07,  1.18s/it]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 12/17 [00:14<00:05,  1.18s/it]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 13/17 [00:15<00:04,  1.18s/it]\n",
      "Loading safetensors checkpoint shards:  82% Completed | 14/17 [00:16<00:03,  1.18s/it]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 15/17 [00:17<00:02,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:35 [loader.py:429] Loading weights took 19.99 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  94% Completed | 16/17 [00:18<00:01,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:35 [gpu_model_runner.py:1176] Model loading took 30.5159 GB and 23.406672 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:19<00:00,  1.14s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:19<00:00,  1.16s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:36 [loader.py:429] Loading weights took 19.78 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:36 [gpu_model_runner.py:1176] Model loading took 30.5159 GB and 24.727743 seconds\n",
      "INFO 05-26 07:42:40 [kv_cache_utils.py:537] GPU KV cache size: 206,672 tokens\n",
      "INFO 05-26 07:42:40 [kv_cache_utils.py:540] Maximum concurrency for 40,960 tokens per request: 5.05x\n",
      "INFO 05-26 07:42:40 [kv_cache_utils.py:537] GPU KV cache size: 206,672 tokens\n",
      "INFO 05-26 07:42:40 [kv_cache_utils.py:540] Maximum concurrency for 40,960 tokens per request: 5.05x\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:50 [custom_all_reduce.py:229] Registering 0 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:50 [custom_all_reduce.py:229] Registering 0 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:50 [gpu_model_runner.py:1499] Graph capturing finished in 10 secs, took 0.11 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:50 [gpu_model_runner.py:1499] Graph capturing finished in 10 secs, took 0.11 GiB\n",
      "INFO 05-26 07:42:50 [core.py:138] init engine (profile, create kv cache, warmup model) took 13.90 seconds\n"
     ]
    }
   ],
   "source": [
    "model = LLM(\"Qwen/Qwen3-32B\", tensor_parallel_size=2, gpu_memory_utilization=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct conditioned prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt_template.format(question=questions[0])},\n",
    "]\n",
    "completion_prompts = []\n",
    "for summarized_trajectory in summarized_trajectories:\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) + summarized_trajectory + \"\\n\\n**Final Answer**\\n\\\\boxed{\"\n",
    "    completion_prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate answers conditioned on the summarized trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 16/16 [00:01<00:00, 10.76it/s, est. speed input: 6288.64 toks/s, output: 43.06 toks/s]\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectory pruning mean accuracy:  0.9375\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(n=1, max_tokens=20, stop=[\"}\"], seed=42, temperature=0.0)\n",
    "request_outputs = model.generate(completion_prompts, sampling_params)\n",
    "responses = [output.text for resquest_output in request_outputs for output in resquest_output.outputs]\n",
    "scores = score([\"\\\\boxed{\" + respose + \"}\" for respose in responses], [answers[0]] * len(responses))\n",
    "\n",
    "print(\"trajectory pruning mean accuracy: \", np.mean([score[\"correct\"] for score in scores]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reason-stability",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

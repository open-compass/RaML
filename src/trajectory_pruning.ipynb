{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Demo For Trajectory Pruning\n",
    "This file shows the basic implementation of the experiments of the trajectory pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/QwQ-32B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load AIME24 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingFaceH4/aime_2024\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Original Trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define verify function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(predictions, references):\n",
    "    try:\n",
    "        from latex2sympy2_extended import NormalizationConfig\n",
    "        from math_verify import (ExprExtractionConfig,\n",
    "                                    LatexExtractionConfig, parse, verify)\n",
    "    except ImportError:\n",
    "        raise ImportError('Failed to import required modules. Please '\n",
    "                            'install the necessary packages: '\n",
    "                            'pip install math_verify latex2sympy2_extended')\n",
    "\n",
    "    correct = 0\n",
    "    count = 0\n",
    "    details = []\n",
    "    for i, j in zip(predictions, references):\n",
    "        count += 1\n",
    "        j_with_env = f'${j}$'\n",
    "        gold_parsed = parse(\n",
    "            j_with_env,\n",
    "            extraction_mode='first_match',\n",
    "            extraction_config=[\n",
    "                LatexExtractionConfig(),\n",
    "                ExprExtractionConfig(),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        if len(gold_parsed) != 0:\n",
    "            # We require the answer to be provided in correct\n",
    "            # latex (no malformed operators)\n",
    "            answer_parsed = parse(\n",
    "                i,\n",
    "                extraction_config=[\n",
    "                    LatexExtractionConfig(\n",
    "                        normalization_config=NormalizationConfig(\n",
    "                            nits=False,\n",
    "                            malformed_operators=False,\n",
    "                            basic_latex=True,\n",
    "                            equations=True,\n",
    "                            boxed='all',\n",
    "                            units=True,\n",
    "                        ),\n",
    "                        # Ensures that boxed is tried first\n",
    "                        boxed_match_priority=0,\n",
    "                        try_extract_without_anchor=False,\n",
    "                    )\n",
    "                ],\n",
    "                extraction_mode='first_match',\n",
    "            )\n",
    "\n",
    "            answer_correct = float(verify(answer_parsed, gold_parsed))\n",
    "            correct += answer_correct\n",
    "            detail = {\n",
    "                'pred': str(answer_parsed),\n",
    "                'answer': str(gold_parsed),\n",
    "                'correct': True if answer_correct else False,\n",
    "            }\n",
    "            details.append(detail)\n",
    "    return details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LLM(\"Qwen/Qwen3-32B\", tensor_parallel_size=2, gpu_memory_utilization=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"{question}\\n\\nPlease reason step by step, and put your final answer within \\\\boxed{{}}\"\n",
    "inputs = []\n",
    "answers = []\n",
    "questions = []\n",
    "for example in dataset:\n",
    "    questions.append(example[\"problem\"])\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt_template.format(question=questions[-1])}\n",
    "    ]\n",
    "    inputs.append(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=True))\n",
    "    answers.append(example[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate response, only one sample for simple demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 16/16 [08:58<00:00, 33.64s/it, est. speed input: 4.70 toks/s, output: 130.23 toks/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    max_tokens=32768,  \n",
    "    temperature=0.6,  \n",
    "    top_p=0.95,\n",
    "    min_p=0.0,\n",
    "    top_k=40,\n",
    "    n=16,\n",
    "    skip_special_tokens=False,\n",
    ")\n",
    "outputs = model.generate(inputs[:1], sampling_params, use_tqdm=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parse and evaluate the trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:18:56] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "trajectories = []\n",
    "references = []\n",
    "for output, answer in zip(outputs, answers[:1]):\n",
    "    prompt = output.prompt\n",
    "    for r in output.outputs:\n",
    "        trajectories.append(r.text)\n",
    "        references.append(answer)\n",
    "scores = score(trajectories, references)\n",
    "\n",
    "print(\"mean accuracy: \", np.mean([score[\"correct\"] for score in scores]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Summarized Trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load summarization model, we utilize Qwen3-32B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 07:22:29 [config.py:583] This model supports multiple tasks: {'generate', 'embed', 'score', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 05-26 07:22:29 [config.py:1515] Defaulting to use mp for distributed inference\n",
      "INFO 05-26 07:22:29 [config.py:1693] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-26 07:22:34 [__init__.py:256] Automatically detected platform cuda.\n",
      "[2025-05-26 07:22:36,005] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "INFO 05-26 07:22:36 [core.py:53] Initializing a V1 LLM engine (v0.8.1) with config: model='Qwen/Qwen2.5-32B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-32B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen2.5-32B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-26 07:22:36 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 05-26 07:22:36 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_07c33891'), local_subscribe_addr='ipc:///tmp/11f960ae-63d2-4fc4-95e3-d691efb28f37', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 05-26 07:22:40 [__init__.py:256] Automatically detected platform cuda.\n",
      "[2025-05-26 07:22:42,037] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "WARNING 05-26 07:22:43 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fd2682a5050>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:22:43 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_c58d7188'), local_subscribe_addr='ipc:///tmp/e42a919d-cebe-453f-a9d4-028a15132d4f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 05-26 07:22:47 [__init__.py:256] Automatically detected platform cuda.\n",
      "[2025-05-26 07:22:48,506] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "WARNING 05-26 07:22:49 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f043100e690>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:22:49 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d541aa50'), local_subscribe_addr='ipc:///tmp/c7474b9b-4bfd-403d-8de6-58cacf627fa7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:22:49 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:22:49 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:22:49 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:22:49 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:22:50 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 05-26 07:22:50 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:22:50 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_0a5c508e'), local_subscribe_addr='ipc:///tmp/64ba8c88-c697-4772-b9ee-ec0b9890d01d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:22:50 [parallel_state.py:967] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-26 07:22:50 [parallel_state.py:967] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:22:50 [cuda.py:215] Using Flash Attention backend on V1 engine.\n",
      "INFO 05-26 07:22:50 [cuda.py:215] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:22:50 [gpu_model_runner.py:1164] Starting to load model Qwen/Qwen2.5-32B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:22:50 [gpu_model_runner.py:1164] Starting to load model Qwen/Qwen2.5-32B-Instruct...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m WARNING 05-26 07:22:50 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m WARNING 05-26 07:22:50 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:22:51 [weight_utils.py:257] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:22:51 [weight_utils.py:257] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:04<01:12,  4.53s/it]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:09<01:11,  4.76s/it]\n",
      "Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:13<01:02,  4.45s/it]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:17<00:57,  4.40s/it]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:22<00:52,  4.35s/it]\n",
      "Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:26<00:47,  4.33s/it]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:30<00:43,  4.30s/it]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:35<00:38,  4.32s/it]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:40<00:36,  4.56s/it]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 10/17 [00:44<00:32,  4.66s/it]\n",
      "Loading safetensors checkpoint shards:  65% Completed | 11/17 [00:49<00:26,  4.47s/it]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 12/17 [00:53<00:22,  4.55s/it]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 13/17 [00:58<00:18,  4.68s/it]\n",
      "Loading safetensors checkpoint shards:  82% Completed | 14/17 [01:03<00:14,  4.69s/it]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 15/17 [01:08<00:09,  4.69s/it]\n",
      "Loading safetensors checkpoint shards:  94% Completed | 16/17 [01:13<00:04,  4.84s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [01:17<00:00,  4.74s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [01:17<00:00,  4.58s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:24:09 [loader.py:429] Loading weights took 77.91 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:24:09 [loader.py:429] Loading weights took 77.35 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:24:10 [gpu_model_runner.py:1176] Model loading took 30.7098 GB and 79.461885 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:24:10 [gpu_model_runner.py:1176] Model loading took 30.7098 GB and 79.465753 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:24:24 [backends.py:409] Using cache directory: /root/.cache/vllm/torch_compile_cache/4f59ca9ae3/rank_0_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:24:24 [backends.py:419] Dynamo bytecode transform time: 14.91 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:24:25 [backends.py:409] Using cache directory: /root/.cache/vllm/torch_compile_cache/4f59ca9ae3/rank_1_0 for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:24:25 [backends.py:419] Dynamo bytecode transform time: 14.95 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:24:28 [backends.py:132] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:24:28 [backends.py:132] Cache the graph of shape None for later use\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:25:16 [backends.py:144] Compiling a graph for general shape takes 50.66 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:25:17 [backends.py:144] Compiling a graph for general shape takes 51.20 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:25:42 [monitor.py:33] torch.compile takes 65.57 s in total\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:25:42 [monitor.py:33] torch.compile takes 66.15 s in total\n",
      "INFO 05-26 07:25:43 [kv_cache_utils.py:537] GPU KV cache size: 146,848 tokens\n",
      "INFO 05-26 07:25:43 [kv_cache_utils.py:540] Maximum concurrency for 32,768 tokens per request: 4.48x\n",
      "INFO 05-26 07:25:43 [kv_cache_utils.py:537] GPU KV cache size: 146,848 tokens\n",
      "INFO 05-26 07:25:43 [kv_cache_utils.py:540] Maximum concurrency for 32,768 tokens per request: 4.48x\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:26:20 [custom_all_reduce.py:229] Registering 8643 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:26:27 [custom_all_reduce.py:229] Registering 8643 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3194147)\u001b[0;0m INFO 05-26 07:26:27 [gpu_model_runner.py:1499] Graph capturing finished in 44 secs, took 3.56 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3194096)\u001b[0;0m INFO 05-26 07:26:27 [gpu_model_runner.py:1499] Graph capturing finished in 44 secs, took 3.56 GiB\n",
      "INFO 05-26 07:26:27 [core.py:138] init engine (profile, create kv cache, warmup model) took 137.88 seconds\n"
     ]
    }
   ],
   "source": [
    "sum_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-32B-Instruct\")\n",
    "sum_model = LLM(\"Qwen/Qwen2.5-32B-Instruct\", trust_remote_code=True, tensor_parallel_size=2, gpu_memory_utilization=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_prompt_template = \"{trajectory}\\n\\n\\nSummarize the aforementioned reasoning process and not explicitly include the final conclusion and answer. Only provide the English summary.\"\n",
    "\n",
    "summarize_messages = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\", \"content\": summarize_prompt_template.format(\n",
    "            # remove answer\n",
    "            trajectory=sum_tokenizer.decode(sum_tokenizer.encode(trajectory.split(\"</think>\")[0].split(\"\\n\\n**Final Answer**\")[0])[:24576])\n",
    "        )},\n",
    "    ] for trajectory in trajectories\n",
    "]\n",
    "summarize_prompts = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True) for message in summarize_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate summarized trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 16/16 [00:31<00:00,  1.94s/it, est. speed input: 1829.23 toks/s, output: 217.19 toks/s]\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(n=1, max_tokens=8192, seed=42, temperature=1.0)\n",
    "request_outputs = sum_model.generate(summarize_prompts, sampling_params, use_tqdm=True) \n",
    "summarized_trajectories = [output.text for resquest_output in request_outputs for output in resquest_output.outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The problem gives two scenarios with Aya walking at different speeds and needing to account for a fixed time t in the coffee shop. The goal is to determine the total time taken when she walks at a speed of s + 1/2 km/h, including the time spent at the coffee shop. The initial steps involve converting the total times given in the scenarios (4 hours and 2 hours and 24 minutes) into equations involving the walking speeds. To create these equations, we use the relationship between speed, distance, and time, where the distance is fixed at 9 km in both cases.\\n\\nThe first scenario's equation is 9/s = 4 - t/60 and the second scenario's equation is 9/(s + 2) = 2.4 - t/60. By subtracting the second equation from the first, the t/60 terms cancel, leaving us with an equation to solve for s. After solving, we find that s = 2.5 km/h. Plugging s back into one of the initial equations allows us to solve for t, giving us t = 24 minutes.\\n\\nFor the final scenario where she walks at s + 1/2 km/h (making the speed 3 km/h), we find the walking time by using the distance-time relationship, which yields 9 km / 3 km/h = 3 hours. Converting 3 hours into minutes (180 minutes) and adding t (24 minutes), we conclude the total time in minutes.\\n\\nThe solution process involves solving the quadratic equation derived from the initial given times to find the speed and subsequently the coffee shop time, and then applying this information to the new scenario to determine the revised total time including the coffee shop time.\\n</think>\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_trajectories[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sum_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Answer Based on the Summarized Trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-26 07:41:50 [config.py:583] This model supports multiple tasks: {'generate', 'embed', 'score', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "INFO 05-26 07:41:50 [config.py:1515] Defaulting to use mp for distributed inference\n",
      "INFO 05-26 07:41:50 [config.py:1693] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 05-26 07:41:56 [__init__.py:256] Automatically detected platform cuda.\n",
      "[2025-05-26 07:41:57,696] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "INFO 05-26 07:41:58 [core.py:53] Initializing a V1 LLM engine (v0.8.1) with config: model='Qwen/Qwen3-32B', speculative_config=None, tokenizer='Qwen/Qwen3-32B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=Qwen/Qwen3-32B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n",
      "WARNING 05-26 07:41:58 [multiproc_worker_utils.py:306] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 05-26 07:41:58 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 10485760, 10, 'psm_09830435'), local_subscribe_addr='ipc:///tmp/3033cb3b-7fab-43cf-b70b-fef730fa2a53', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 05-26 07:42:02 [__init__.py:256] Automatically detected platform cuda.\n",
      "[2025-05-26 07:42:03,785] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "WARNING 05-26 07:42:04 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f434e26d050>\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:04 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d201ed15'), local_subscribe_addr='ipc:///tmp/65314d8e-b3ba-4e0a-94ac-29e3ad5100fc', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 05-26 07:42:08 [__init__.py:256] Automatically detected platform cuda.\n",
      "[2025-05-26 07:42:10,189] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "WARNING 05-26 07:42:11 [utils.py:2282] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fce5eff3c50>\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:11 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0d32e852'), local_subscribe_addr='ipc:///tmp/b783c30e-41dd-4b5a-a5ae-fe064ed2f00e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:11 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:11 [utils.py:925] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:11 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:11 [pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:11 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:11 [custom_all_reduce_utils.py:244] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:11 [shm_broadcast.py:258] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_7879d013'), local_subscribe_addr='ipc:///tmp/a9cef19b-5fa1-4dd9-8fdc-efab1737a28b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:11 [parallel_state.py:967] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:11 [parallel_state.py:967] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:11 [cuda.py:215] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:11 [cuda.py:215] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:11 [gpu_model_runner.py:1164] Starting to load model Qwen/Qwen3-32B...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:11 [gpu_model_runner.py:1164] Starting to load model Qwen/Qwen3-32B...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m WARNING 05-26 07:42:11 [utils.py:79] Qwen3ForCausalLM has no vLLM implementation, falling back to Transformers implementation. Some features may not be supported and performance may not be optimal.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m WARNING 05-26 07:42:11 [utils.py:79] Qwen3ForCausalLM has no vLLM implementation, falling back to Transformers implementation. Some features may not be supported and performance may not be optimal.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:11 [transformers.py:115] Using Transformers backend.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:11 [transformers.py:115] Using Transformers backend.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m WARNING 05-26 07:42:12 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m WARNING 05-26 07:42:12 [config.py:3670] `torch.compile` is turned on, but the model Qwen/Qwen3-32B does not support it. Please open an issue on GitHub if you want it to be supported.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m WARNING 05-26 07:42:12 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m WARNING 05-26 07:42:12 [config.py:3670] `torch.compile` is turned on, but the model Qwen/Qwen3-32B does not support it. Please open an issue on GitHub if you want it to be supported.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:14 [weight_utils.py:257] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:15 [weight_utils.py:257] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 1/17 [00:01<00:19,  1.21s/it]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 2/17 [00:02<00:18,  1.22s/it]\n",
      "Loading safetensors checkpoint shards:  18% Completed | 3/17 [00:03<00:14,  1.05s/it]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 4/17 [00:04<00:14,  1.10s/it]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 5/17 [00:05<00:13,  1.15s/it]\n",
      "Loading safetensors checkpoint shards:  35% Completed | 6/17 [00:06<00:12,  1.17s/it]\n",
      "Loading safetensors checkpoint shards:  41% Completed | 7/17 [00:08<00:11,  1.19s/it]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 8/17 [00:09<00:10,  1.19s/it]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 9/17 [00:10<00:09,  1.19s/it]\n",
      "Loading safetensors checkpoint shards:  59% Completed | 10/17 [00:11<00:08,  1.18s/it]\n",
      "Loading safetensors checkpoint shards:  65% Completed | 11/17 [00:12<00:07,  1.18s/it]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 12/17 [00:14<00:05,  1.18s/it]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 13/17 [00:15<00:04,  1.18s/it]\n",
      "Loading safetensors checkpoint shards:  82% Completed | 14/17 [00:16<00:03,  1.18s/it]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 15/17 [00:17<00:02,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:35 [loader.py:429] Loading weights took 19.99 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  94% Completed | 16/17 [00:18<00:01,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:35 [gpu_model_runner.py:1176] Model loading took 30.5159 GB and 23.406672 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:19<00:00,  1.14s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 17/17 [00:19<00:00,  1.16s/it]\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:36 [loader.py:429] Loading weights took 19.78 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:36 [gpu_model_runner.py:1176] Model loading took 30.5159 GB and 24.727743 seconds\n",
      "INFO 05-26 07:42:40 [kv_cache_utils.py:537] GPU KV cache size: 206,672 tokens\n",
      "INFO 05-26 07:42:40 [kv_cache_utils.py:540] Maximum concurrency for 40,960 tokens per request: 5.05x\n",
      "INFO 05-26 07:42:40 [kv_cache_utils.py:537] GPU KV cache size: 206,672 tokens\n",
      "INFO 05-26 07:42:40 [kv_cache_utils.py:540] Maximum concurrency for 40,960 tokens per request: 5.05x\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:50 [custom_all_reduce.py:229] Registering 0 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:50 [custom_all_reduce.py:229] Registering 0 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=3199230)\u001b[0;0m INFO 05-26 07:42:50 [gpu_model_runner.py:1499] Graph capturing finished in 10 secs, took 0.11 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=3199191)\u001b[0;0m INFO 05-26 07:42:50 [gpu_model_runner.py:1499] Graph capturing finished in 10 secs, took 0.11 GiB\n",
      "INFO 05-26 07:42:50 [core.py:138] init engine (profile, create kv cache, warmup model) took 13.90 seconds\n"
     ]
    }
   ],
   "source": [
    "model = LLM(\"Qwen/Qwen3-32B\", tensor_parallel_size=2, gpu_memory_utilization=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construct conditioned prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt_template.format(question=questions[0])},\n",
    "]\n",
    "completion_prompts = []\n",
    "for summarized_trajectory in summarized_trajectories:\n",
    "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) + summarized_trajectory + \"\\n\\n**Final Answer**\\n\\\\boxed{\"\n",
    "    completion_prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate answers conditioned on the summarized trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 16/16 [00:01<00:00, 10.76it/s, est. speed input: 6288.64 toks/s, output: 43.06 toks/s]\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n",
      "[2025-05-26 07:47:25] WARNING math_normalization.py:466: equations is deprecated, as it handled by the parser now\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trajectory pruning mean accuracy:  0.9375\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(n=1, max_tokens=20, stop=[\"}\"], seed=42, temperature=0.0)\n",
    "request_outputs = model.generate(completion_prompts, sampling_params)\n",
    "responses = [output.text for resquest_output in request_outputs for output in resquest_output.outputs]\n",
    "scores = score([\"\\\\boxed{\" + respose + \"}\" for respose in responses], [answers[0]] * len(responses))\n",
    "\n",
    "print(\"trajectory pruning mean accuracy: \", np.mean([score[\"correct\"] for score in scores]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reason-stability",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

# RaML: Deciphering Trajectory-Aided LLM Reasoning from an Optimization Perspective


<p align="center">
ðŸ¤— <a href="https://huggingface.co/collections/jnanliu/raml-data-6834222cdc8b182b0d50c62e">Dataset</a>&nbsp;&nbsp;|&nbsp;&nbsp;ðŸ“„ <a href="https://arxiv.org/abs/2505.19815">arXiv</a>
</p>

> *This repository provides the codebase utilized by the paper **Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective***

![Framework Overview](assets/framework.png)

## ðŸ“‹ Introduction

We introduce a novel framework for interpreting the reasoning capabilities of large language models (LLMs) through the lens of meta-learning. By conceptualizing reasoning trajectories as pseudo-gradient descent updates to the LLMâ€™s parameters, we identify parallels between LLM reasoning and various meta-learning paradigms. We formalize the training process for reasoning tasks as a meta-learning setup, with each question treated as an individual task, and reasoning trajectories serving as the inner loop optimization for adapting model parameters. Once trained on a diverse set of questions, the LLM develops fundamental reasoning capabilities that can generalize to previously unseen questions. Extensive empirical evaluations substantiate the strong connection between LLM reasoning and meta-learning, exploring several issues of significant interest from a meta-learning standpoint. Our work not only enhances the understanding of LLM reasoning but also provides practical insights for improving these models through established meta-learning techniques.

## ðŸ§° Dependencies

* ðŸ Python 3.10
* âš¡ PyTorch 2.6.0
* ðŸ§  veRL
* ðŸš€ vLLM
* ðŸ“Š OpenCompass

## ðŸ“ Code Organization

### ðŸ“š Data

We provide the reasoning trajectories generated by Qwen2.5-72B-Instruct and DeepSeek-Distill-Qwen-14B on Hugging Face:

* [jnanliu/orz-math-filtered-qwen-72b-rollout](https://huggingface.co/datasets/jnanliu/orz-math-filtered-qwen-72b-rollout)
* [jnanliu/orz-math-filtered-distill-14b-rollout](https://huggingface.co/datasets/jnanliu/orz-math-filtered-distill-14b-rollout)

### ðŸŽ“ SFT & GRPO Training

We utilize veRL to conduct training, you can refer to its [github](https://github.com/volcengine/verl) for launching training. 

The relevant code is located in `src/third_party/verl`:

```
ðŸ“¦ verl
 â”£ ðŸ“‚trainer
 â”ƒ â”£ ðŸ“‚config
 â”ƒ â”ƒ â”— ðŸ“œ ppo_trainer.yaml
 â”ƒ â”— ðŸ“‚ppo
 â”ƒ â”ƒ â”— ðŸ“œ ray_trainer.py
 â”£ ðŸ“‚workers
 â”ƒ â”— ðŸ“‚actor
 â”ƒ â”ƒ â”— ðŸ“œ dp_actor.py
 â”£ ðŸ“œ data_process_grpo.py
 â”— ðŸ“œ data_process_sft.py
```

#### ðŸ› ï¸ Data Processing Scripts

* `data_process_sft.py`: Script to generate Parquet files for SFT training.
* `data_process_grpo.py`: Script to generate Parquet files for GRPO training.

#### âš™ï¸ GRPO Training Configuration

The GRPO training implementation is detailed in:

* `verl/trainer/ppo/ray_trainer.py`
* `verl/workers/actor/dp_actor.py`
* `verl/trainer/config/ppo_trainer.yaml`

Specifically, we generate the `gen_mask` to control the number of update trajectories per question and add it to the `batch` in lines 867â€“881 of `ray_trainer.py`:

```python
from collections import defaultdict
index = batch.non_tensor_batch['uid']
prompt2sample = defaultdict(list)
for i in range(len(index)):
    prompt2sample[index[i]].append(i)
responses = batch.batch['responses']
response_length = responses.size(1)
attention_mask = batch.batch['attention_mask']
response_mask = attention_mask[:, -response_length:]
mask = torch.zeros_like(response_mask)
import random
for indices in prompt2sample.values():
    for i in random.sample(indices, self.config.actor_rollout_ref.actor.num_update_sample_per_prompt):
        mask[i] = 1
batch.batch.update({"gen_mask": mask})
```

Then, in line 279 of `dp_actor.py`, we obtain the `gen_mask`:

```python
gen_mask = data['gen_mask']
```

We combine the `gen_mask` and `response_mask` in lines 291â€“296 of `dp_actor.py`:

```python
pg_loss, pg_clipfrac, ppo_kl = core_algos.compute_policy_loss(
    old_log_prob=old_log_prob,
    log_prob=log_prob,
    advantages=advantages,
    eos_mask=response_mask & gen_mask,
    cliprange_min=clip_ratio_min,
    cliprange_max=clip_ratio_max
)
```

We also add the configuration parameter `num_update_sample_per_prompt` in `ppo_trainer.yaml`.

> For the SFT experiments, you can control the number of update trajectories per question in `data_process_sft.py` by adjusting the training trajectories.

### ðŸ§ª Evaluation on Benchmarks

We utilize [OpenCompass](https://github.com/open-compass/opencompass) for evaluation. A demo evaluation config file is provided in `src/third_party/opencompass/evaluation_config.py`.

```
ðŸ“¦ opencompass
 â”— ðŸ“œ evaluation_config.py
```

Please refer to the [OpenCompass documentation](https://opencompass.readthedocs.io/en/latest/) for details on running the evaluation.

### ðŸ“Š Analysis Notebooks

We provide several demo notebooks in `src/` for experiments discussed in the paper:

```
ðŸ“¦ src
 â”£ ðŸ“‚third_party
 â”ƒ â”£ ðŸ“‚opencompass
 â”ƒ â”— ðŸ“‚verl
 â”£ ðŸ“œ landscape.ipynb                # Visualizing landscape
 â”£ ðŸ“œ pseudo_gradient_descent.ipynb  # Experiments on pseudo gradient descent
 â”— ðŸ“œ trajectory_pruning.ipynb       # Experiments on trajectory pruning
```

## ðŸ“– Citation

If you find this repository helpful, please consider citing our paper:

```
@article{abs-2505-19815,
  author       = {Junnan Liu and Hongwei Liu and Linchen Xiao and Shudong Liu and Taolin Zhang and Zihan Ma and Songyang Zhang and Kai Chen},
  title        = {s1: Simple test-time scaling},
  journal      = {CoRR},
  volume       = {abs/2505.19815},
  year         = {2025}
}
```

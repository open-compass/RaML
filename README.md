# RaML: Deciphering Trajectory-Aided LLM Reasoning from an Optimization Perspective


<p align="center">
ğŸ¤— <a href="https://huggingface.co/collections/jnanliu/raml-data-6834222cdc8b182b0d50c62e">Dataset</a>&nbsp;&nbsp;|&nbsp;&nbsp;ğŸ“„ <a href="">arXiv</a>
</p>

> *This repository provides the codebase utilized by the paper **Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective***

![Framework Overview](assets/framework.png)

## ğŸ“‹ Introduction

We introduce a novel framework for interpreting the reasoning capabilities of large language models (LLMs) through the lens of meta-learning. By conceptualizing reasoning trajectories as pseudo-gradient descent updates to the LLMâ€™s parameters, we identify parallels between LLM reasoning and various meta-learning paradigms. We formalize the training process for reasoning tasks as a meta-learning setup, with each question treated as an individual task, and reasoning trajectories serving as the inner loop optimization for adapting model parameters. Once trained on a diverse set of questions, the LLM develops fundamental reasoning capabilities that can generalize to previously unseen questions. Extensive empirical evaluations substantiate the strong connection between LLM reasoning and meta-learning, exploring several issues of significant interest from a meta-learning standpoint. Our work not only enhances the understanding of LLM reasoning but also provides practical insights for improving these models through established meta-learning techniques.

## ğŸ§° Dependencies

* ğŸ Python 3.10
* âš¡ PyTorch 2.6.0
* ğŸ§  veRL
* ğŸš€ vLLM
* ğŸ“Š OpenCompass

## ğŸ“ Code Organization

### ğŸ“š Data

We provide the reasoning trajectories generated by Qwen2.5-72B-Instruct and DeepSeek-Distill-Qwen-14B on Hugging Face:

* [jnanliu/orz-math-filtered-qwen-72b-rollout](https://huggingface.co/datasets/jnanliu/orz-math-filtered-qwen-72b-rollout)
* [jnanliu/orz-math-filtered-distill-14b-rollout](https://huggingface.co/datasets/jnanliu/orz-math-filtered-distill-14b-rollout)

### ğŸ“ SFT & GRPO Training

We utilize veRL to conduct training, you can refer to its [github](https://github.com/volcengine/verl) for launching training. 

The relevant code is located in `src/third_party/verl`:

```
ğŸ“¦ verl
 â”£ ğŸ“‚trainer
 â”ƒ â”£ ğŸ“‚config
 â”ƒ â”ƒ â”— ğŸ“œ ppo_trainer.yaml
 â”ƒ â”— ğŸ“‚ppo
 â”ƒ â”ƒ â”— ğŸ“œ ray_trainer.py
 â”£ ğŸ“‚workers
 â”ƒ â”— ğŸ“‚actor
 â”ƒ â”ƒ â”— ğŸ“œ dp_actor.py
 â”£ ğŸ“œ data_process_grpo.py
 â”— ğŸ“œ data_process_sft.py
```

#### ğŸ› ï¸ Data Processing Scripts

* `data_process_sft.py`: Script to generate Parquet files for SFT training.
* `data_process_grpo.py`: Script to generate Parquet files for GRPO training.

#### âš™ï¸ GRPO Training Configuration

The GRPO training implementation is detailed in:

* `verl/trainer/ppo/ray_trainer.py`
* `verl/workers/actor/dp_actor.py`
* `verl/trainer/config/ppo_trainer.yaml`

Specifically, we generate the `gen_mask` to control the number of update trajectories per question and add it to the `batch` in lines 867â€“881 of `ray_trainer.py`:

```python
from collections import defaultdict
index = batch.non_tensor_batch['uid']
prompt2sample = defaultdict(list)
for i in range(len(index)):
    prompt2sample[index[i]].append(i)
responses = batch.batch['responses']
response_length = responses.size(1)
attention_mask = batch.batch['attention_mask']
response_mask = attention_mask[:, -response_length:]
mask = torch.zeros_like(response_mask)
import random
for indices in prompt2sample.values():
    for i in random.sample(indices, self.config.actor_rollout_ref.actor.num_update_sample_per_prompt):
        mask[i] = 1
batch.batch.update({"gen_mask": mask})
```

Then, in line 279 of `dp_actor.py`, we obtain the `gen_mask`:

```python
gen_mask = data['gen_mask']
```

We combine the `gen_mask` and `response_mask` in lines 291â€“296 of `dp_actor.py`:

```python
pg_loss, pg_clipfrac, ppo_kl = core_algos.compute_policy_loss(
    old_log_prob=old_log_prob,
    log_prob=log_prob,
    advantages=advantages,
    eos_mask=response_mask & gen_mask,
    cliprange_min=clip_ratio_min,
    cliprange_max=clip_ratio_max
)
```

We also add the configuration parameter `num_update_sample_per_prompt` in `ppo_trainer.yaml`.

> For the SFT experiments, you can control the number of update trajectories per question in `data_process_sft.py` by adjusting the training trajectories.

### ğŸ§ª Evaluation on Benchmarks

We utilize [OpenCompass](https://github.com/open-compass/opencompass) for evaluation. A demo evaluation config file is provided in `src/third_party/opencompass/evaluation_config.py`.

```
ğŸ“¦ opencompass
 â”— ğŸ“œ evaluation_config.py
```

Please refer to the [OpenCompass documentation](https://opencompass.readthedocs.io/en/latest/) for details on running the evaluation.

### ğŸ“Š Analysis Notebooks

We provide several demo notebooks in `src/` for experiments discussed in the paper:

```
ğŸ“¦ src
 â”£ ğŸ“‚third_party
 â”ƒ â”£ ğŸ“‚opencompass
 â”ƒ â”— ğŸ“‚verl
 â”£ ğŸ“œ landscape.ipynb                # Visualizing landscape
 â”£ ğŸ“œ pseudo_gradient_descent.ipynb  # Experiments on pseudo gradient descent
 â”— ğŸ“œ trajectory_pruning.ipynb       # Experiments on trajectory pruning
```

## ğŸ“– Citation

If you find this repository helpful, please consider citing our paper:

```

```

